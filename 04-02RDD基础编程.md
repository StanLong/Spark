# 基础编程

在 Spark 中创建RDD 的创建方式可以分为四种

- 从集合（内存）中创建 RDD
- 从外部存储（文件）创建RDD
- 从其他 RDD 创建
- 直接创建 RDD（new）

## 从集合（内存）中创建 RDD

```scala
package com.stanlong.spark.core.rdd.builder

import org.apache.spark.{SparkConf, SparkContext}

object Spark01_RDD_Memory {

    def main(args: Array[String]): Unit = {
        // 准备环境
        val spakConf = new SparkConf().setMaster("local[*]").setAppName("RDD") // [*] 表示当前系统最大可用核数，如果省略则表示用单线程模拟单核
        val sc = new SparkContext(spakConf)

        // 创建RDD
        //从内存中创建RDD 将内存中集合的数据作为处理的数据源
        val seq = Seq[Int](1,2,3,4)
        // val rdd = sc.parallelize(seq) // parallelize 并行, 等同于下面一行
        val rdd = sc.makeRDD(seq) // makeRDD 的底层实现也是调用了 rdd对象的parallelize方法

        rdd.collect().foreach(println)

        // 关闭环境
        sc.stop()
    }
}
```

## 从外部存储（文件）创建RDD

由外部存储系统的数据集创建RDD 包括：本地的文件系统，所有Hadoop 支持的数据集， 比如HDFS、HBase 等

```scala
package com.stanlong.spark.core.rdd.builder

import org.apache.spark.{SparkConf, SparkContext}

object Spark02_RDD_File {

    def main(args: Array[String]): Unit = {
        // 准备环境
        val spakConf = new SparkConf().setMaster("local[*]").setAppName("RDD") // [*] 表示当前系统最大可用核数，如果省略则表示用单线程模拟单核
        val sc = new SparkContext(spakConf)

        // 创建RDD
        //从文件中创建RDD 将文件中的数据作为处理的数据源
        // textFile 以行为单位读取数据，读取到的都是字符串
        val rdd = sc.textFile("datas/1.txt") // 路径默认以当前环境的根路径为基准，也可用写绝对路径。如果文件后缀名相同，也可以使用通配符

        // wholeTextFiles 以文件为单位读取数据，读取的结果是个元组，第一个元素表示路径，第二个元素表示文件内容
        // val rdd = sc.wholeTextFiles("datas")

        rdd.collect().foreach(println)

        // 关闭环境
        sc.stop()
    }
}
```

## 从其他 RDD 创建

主要是通过一个RDD 运算完后，再产生新的RDD。详情请参考后续章节

## 直接创建 RDD（new）

使用 new 的方式直接构造RDD，一般由Spark 框架自身使用。

# RDD 并行度与分区

默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD 时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。

## 集合数据源分区数据的分配

```scala
package com.stanlong.spark.core.rdd.builder

import org.apache.spark.{SparkConf, SparkContext}
/*
 * 从集合中读取数据并分区
 */
object Spark03_RDD_Memory_Par {

    def main(args: Array[String]): Unit = {
        // 准备环境
        val spakConf = new SparkConf().setMaster("local[*]").setAppName("RDD") // [*] 表示当前系统最大可用核数，如果省略则表示用单线程模拟单核

        // spakConf.set("spark.default.parallelism", "5") // 配置默认并行度

        val sc = new SparkContext(spakConf)

        // 创建RDD
        // 第一个参数表示集合序列，
        // 第二个参数表示分区的数量，不传表示使用默认值，默认值和CPU核数相同。
        val rdd = sc.makeRDD(List(1,2,3,4), 2)

        // 将处理的数据保存成分区文件
        rdd.saveAsTextFile("output")

        // 关闭环境
        sc.stop()

    }
}
```

**output 目录内容**

![](D:/StanLong/git_repository/Spark/doc/38.png)



```scala
// 计算数据分区位置的源码
(0 until numSlices).iterator.map{ i =>
    val start = ((i * length)/numSlices).toInt
    val end = (((i+1) * length)/numSlices).toInt
    (start, end)
}
------------------------------------------------------------------------

// 以 rdd = sc.makeRDD(List(1,2,3,4), 2) 的分区数据为例
// part-00000 中的数据是 1, 2 。 part-00001 中的数据是3,4
// 套用源码，length=4, numSlices = 2
        
List(1,2,3,4), 2
// 则
length = 4, numSlices=2

// 分区一
start = 0 * 4 / 2 = 0
end = 1 * 4 / 2 = 2

// 分区二
start = 1 * 4 / 2 = 2
end = 2 * 4 / 2 = 4


0 => (0, 2) => List.slice(0, 2) => (1,2)
1 => (2, 4) => List.slice(2, 4) => (3,4)
```

## 文件数据源分区数据的分配

1.txt

```
Hello World
Hello Scala
```

```scala
package com.stanlong.spark.core.rdd.builder

import org.apache.spark.{SparkConf, SparkContext}

/*
 * 从文件中读取数据并分区
 */
object Spark03_RDD_File_Par {

    def main(args: Array[String]): Unit = {
        // 准备环境
        val spakConf = new SparkConf().setMaster("local[*]").setAppName("RDD") // [*] 表示当前系统最大可用核数，如果省略则表示用单线程模拟单核

        val sc = new SparkContext(spakConf)

        // textFile 默认两个分区
        val rdd = sc.textFile("datas/1.txt")
        // val rdd = sc.textFile("datas/1.txt", 3) 也可以手动指定分区数，比如指定三个分区

        // 将处理的数据保存成分区文件
        rdd.saveAsTextFile("output")

        // 关闭环境
        sc.stop()
    }
}
```

```scala
// 分区数量的计算方式
// 文件 totalSize
// goalSize = totalSize / (numSplits == 0 ? 1 : numSplits) 分区数量，如果余数大于商的10%， 则会新增一个分区
// 24/3 = 8(字节)
// 24/8=3..0(分区)

// 分区数据的计算方式
// 1. spark 读取文件，采用的是hadoop方式读取，所以一行一行的读，和字节数没有关系
// 2. 读取数据时以偏移量为单位， 偏移量不会被重新读取
/**
 * Hello World@@ => 偏移量 0-12
 * Hello Scala => 偏移量 13-23
 */
// 3. 数据分区的偏移量范围的计算
// 0 => [0, 8] =》 Hello World@@
// 1 => [8, 16] =》Hello Scala
// 2 => [16,24] =》空
```

