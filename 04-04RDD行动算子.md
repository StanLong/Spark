# RDD 行动算子

所谓行动算子，其实就是触发作业执行的方法，底层代码调用的是环境对象的 runjob 方法

## 常见行动算子

```scala
package com.stanlong.spark.core.rdd.operator.transform

import org.apache.spark.{SparkConf, SparkContext}

object Spark01_RDD_Operator_Transform {

    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
        val sc = new SparkContext(sparkConf)
        
        val rdd = sc.makeRDD(List(1, 2, 3, 4))
        
        // reduce 聚合
        val result = rdd.reduce(_ + _)
        println(result)
        
        // collect 方法会将不同分区的数据按照分区的顺序采集到driver端内存中，形成数组
        val ints = rdd.collect()
        println(ints.mkString(","))
        
        // count 统计数据源中数据的个数
        val l = rdd.count()
        println(l)
        
        // first 获取数据源中的第一个
        val i = rdd.first()
        println(i)
        
        // take 获取N个数据
        val ints1 = rdd.take(3)
        println(ints1.mkString(","))
        
        // takeOrdered 先排序在取N个数据, 默认升序
        val ints2 = rdd.takeOrdered(3)
        println(ints2.mkString(","))
        
        // aggregateByKey 只会参与分区内计算
        // aggregate 会参与分区内和分区间的运算
        val result1 = rdd.aggregate(10)(_ + _, _ + _)
        println(result1)
        
        // fold 当分区内和分区间的计算规则相同时aggregate的简化写法
        val result2 = rdd.fold(10)(_ + _)
        print(result2)
        
        // countByValue 统计每个值出现的次数
        val rdd1 = sc.makeRDD(List(1, 1, 1, 4), 2)
        val result3 = rdd1.countByValue()
        println(result3)
        
        // countByKey 统计key出现的次数
        val rdd2 = sc.makeRDD(List(("a", 1),("a", 2),("a" ,3),("b", 1)))
        val result4 = rdd2.countByKey()
        println(result4)

        sc.stop()
    }
}
```

## save 相关算子

函数签名: 

```scala
def saveAsTextFile(path: String): Unit 
def saveAsObjectFile(path: String): Unit 

def saveAsSequenceFile(
path: String,
codec: Option[Class[_ <: CompressionCodec]] = None
): Unit
```

函数说明: 将数据保存到不同格式的文件中

实例:

```scala
val rdd = sc.makeRDD(List(("a", 1), ("a", 2), ("a", 3), ("b", 1)), 2)
rdd.saveAsTextFile("output")
rdd.saveAsObjectFile("output1")
// saveAsSequenceFile 要求数据格式必须时key-value类型
rdd.saveAsSequenceFile("output2")
```

**RDD文件读取与保存**

Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。文件格式分为：text 文件、csv 文件、sequence 文件以及Object 文件；
文件系统分为：本地文件系统、HDFS、HBASE 以及数据库。

- text 文件

- sequence 文件

  SequenceFile 文件是Hadoop 用来存储二进制形式的key-value 对而设计的一种平面文件(Flat File)。在 SparkContext 中，可以调用sequenceFile[keyClass, valueClass](path)。

- object 对象文件
  对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制。可以通过objectFile[T: ClassTag](path)函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用saveAsObjectFile()实现

## foreach

函数签名:

```scala
def foreach(f: T => Unit): Unit = withScope { 
    val cleanF = sc.clean(f)
	sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)
)
```

函数说明: 分布式遍历RDD 中的每一个元素，调用指定函数

实例:

```scala
val rdd = sc.makeRDD(List(1,2,3,4))

// foreach 其实是 Driver端内存集合的循环遍历方法
rdd.collect().foreach(println)

println("*****************")

// foreach 其实是 Executor端内存数据打印
rdd.foreach(println)
```

foreach 小练习

```scala
package com.stanlong.spark.core.rdd.operator.action

import org.apache.spark.{SparkConf, SparkContext}

object Spark01_RDD_Operator_Action {
    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
        val sc = new SparkContext(sparkConf)

        val rdd = sc.makeRDD(List(1,2,3,4))
        var user = new User()
        rdd.foreach(
            num =>{
                println("age=" + (user.age + num))
            }
        )
        sc.stop()
    }
}

// 方式一
class User extends Serializable{
    var age = 30
}

// 样例类 样例类在编译时会自动混入序列化特质
// case class User() {
//     var age = 30
// }
```

